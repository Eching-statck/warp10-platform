//
//   Copyright 2019-2022  SenX S.A.S.
//
//   Licensed under the Apache License, Version 2.0 (the "License");
//   you may not use this file except in compliance with the License.
//   You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
//   Unless required by applicable law or agreed to in writing, software
//   distributed under the License is distributed on an "AS IS" BASIS,
//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//   See the License for the specific language governing permissions and
//   limitations under the License.
//

/////////////////////////////////////////////////////////////////////////////////////////
//
// S T O R E
//
/////////////////////////////////////////////////////////////////////////////////////////

###
### M A N D A T O R Y   C O N F I G U R A T I O N
###

##
## MANDATORY - Path to the FoundationDB cluster file. This file needs to be readable AND writable by the
## user running the Store instance.
##
store.fdb.clusterfile = ${fdb.clusterfile}

##
## MANDATORY - Maximum byte size of the pending mutations list before it is flushed to FoundationDB.
## This value cannot exceed 9500000 bytes.
##
store.fdb.data.pendingmutations.maxsize = 5000000

##
## MANDATORY - How many threads to spawn for consuming
## Each of these threads will commit data to FoundationDB
##
store.nthreads = 2

##
## MANDATORY - Comma separated list of Kafka brokers for the data cluster
##
store.kafka.data.consumer.bootstrap.servers = ${kafka.data.bootstrap.servers}

##
## MANDATORY - Actual 'data' topic
##
store.kafka.data.topic = ${kafka.data.topic}

##
## MANDATORY - Kafka group id with which to consume the data topic
##
store.kafka.data.groupid = store.data

##
## MANDATORY - Delay (in ms) between synchronizations for offset commit
##
store.kafka.data.commitperiod = 1000

##
## MANDATORY - Maximum time (in ms) between offset synchronizations - MUST be set to a value above that of store.kafka.data.commitperiod
## This parameter is there to detect calls to FoundationDB which hang.
## The value of this parameter must be set to a value longer than the longest running call to FoundationDB commit, otherwise
## valid operations might not finish.
## Consider it as the time it takes to detect FoundationDB failures. Values of 60000 to 120000 seem good starting points.
##
store.kafka.data.intercommits.maxtime = 120000

###
### O P T I O N A L   C O N F I G U R A T I O N
###
### Values shown are the default when they exist
###

##
## Optional FoundationDB tenant to use.
## The tenant MUST exist.
##
#store.fdb.tenant =

##
## Number of retries for FoundationDB transactions (defaults to 4)
##
#store.fdb.retrylimit = 4

##
## 128/192/256 bits AES key for encrypting data in FoundationDB.
## Valid formats are hex:..., base64:... or, when using OSS, wrapped:....
##
#store.fdb.data.aes = ${fdb.data.aes}

##
## How many threads under each of 'store.nthreads' should consume Kafka.
## Defaults to 1 if unset.
##
#store.nthreads.kafka = 1

##
## Throttling file path. This file contains a single line with a double number between 0.0 and 1.0 wich is interpreted as a rate
## When processing messages, a random number is generated for each one, if the value is >= the rate, then the processing thread waits for store.throttling.delay nanoseconds
##
#store.throttling.file =

##
## How much to wait when the consumption was throttled, in ns (nanoseconds), defaults to 10 ms (milliseconds)
##
#store.throttling.delay = 10000000

##
## How often (in ms) to reread the throttling file (in ms, defaults to 60000).
##
#store.throttling.period = 60000

##
## 128 bits SipHash key for computing MACs. Valid formats are hex:..., base64:... or, when using OSS, wrapped:....
##
#store.kafka.data.mac = ${kafka.data.mac}

##
## 128/192/256 bits AES key for encrypting data in Kafka.
## Valid formats are hex:..., base64:... or, when using OSS, wrapped:....
##
#store.kafka.data.aes = ${kafka.data.aes}

##
## Prefix used to specify cusom Kafka configuration properties
##
#store.kafka.data.consumer.conf.prefix =

##
## Kafka client id to use for the data producer
##
#store.kafka.data.producer.clientid =

##
## A prefix prepended to the Kafka ConsumerId
##
#store.kafka.data.consumerid.prefix =

##
## Name of partition assignment strategy to use
##
#store.kafka.data.consumer.partition.assignment.strategy = 
